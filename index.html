<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Local ML Training Platform</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/tensorflow/4.10.0/tf.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jszip/3.10.1/jszip.min.js"></script>
    <style>
        /* CSS Styles (mostly unchanged, keeping it concise for brevity) */
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); min-height: 100vh; color: #333; }
        .container { max-width: 1200px; margin: 0 auto; padding: 20px; }
        .header { text-align: center; color: white; margin-bottom: 30px; }
        .header h1 { font-size: 2.5rem; margin-bottom: 10px; text-shadow: 2px 2px 4px rgba(0,0,0,0.3); }
        .tabs { display: flex; background: rgba(255,255,255,0.1); border-radius: 15px; padding: 5px; margin-bottom: 30px; backdrop-filter: blur(10px); }
        .tab { flex: 1; padding: 15px; text-align: center; border-radius: 10px; cursor: pointer; color: white; font-weight: 600; transition: all 0.3s ease; }
        .tab.active { background: rgba(255,255,255,0.2); box-shadow: 0 4px 15px rgba(0,0,0,0.1); }
        .tab-content { display: none; background: white; border-radius: 20px; padding: 30px; box-shadow: 0 20px 40px rgba(0,0,0,0.1); backdrop-filter: blur(20px); }
        .tab-content.active { display: block; animation: fadeIn 0.5s ease; }
        @keyframes fadeIn { from { opacity: 0; transform: translateY(20px); } to { opacity: 1; transform: translateY(0); } }
        .file-upload-area { margin-bottom: 20px; }
        .file-upload { border: 3px dashed #667eea; border-radius: 15px; padding: 20px; text-align: center; cursor: pointer; transition: all 0.3s ease; margin-bottom: 10px; }
        .file-upload:hover { border-color: #764ba2; background: rgba(102, 126, 234, 0.05); }
        .file-upload.dragover { background: rgba(102, 126, 234, 0.1); border-color: #764ba2; }
        .file-upload p { font-size: 0.9em; color: #555; }
        .btn { background: linear-gradient(45deg, #667eea, #764ba2); color: white; border: none; padding: 12px 25px; border-radius: 25px; cursor: pointer; font-weight: 600; transition: all 0.3s ease; margin: 5px; }
        .btn:hover { transform: translateY(-2px); box-shadow: 0 10px 20px rgba(0,0,0,0.2); }
        .btn:disabled { opacity: 0.5; cursor: not-allowed; transform: none; }
        .image-grid { display: grid; grid-template-columns: repeat(auto-fill, minmax(200px, 1fr)); gap: 20px; margin-top: 20px; }
        .image-item { border: 2px solid #e0e0e0; border-radius: 10px; overflow: hidden; transition: all 0.3s ease; background: white; position: relative; }
        .annotation-count-badge { position: absolute; top: 5px; right: 5px; background-color: rgba(102, 126, 234, 0.8); color: white; padding: 3px 7px; border-radius: 10px; font-size: 0.8em; font-weight: bold; }
        .image-item:hover { transform: translateY(-5px); box-shadow: 0 10px 20px rgba(0,0,0,0.1); }
        .image-item img { width: 100%; height: 150px; object-fit: cover; }
        .image-controls { padding: 15px; text-align: center; }
        .progress-bar { width: 100%; height: 20px; background: #e0e0e0; border-radius: 10px; overflow: hidden; margin: 20px 0; }
        .progress-fill { height: 100%; background: linear-gradient(45deg, #667eea, #764ba2); width: 0%; transition: width 0.3s ease; }
        .model-info { background: #f8f9fa; padding: 20px; border-radius: 10px; margin: 20px 0; }
        .log { background: #1e1e1e; color: #00dd00; padding: 15px; border-radius: 10px; height: 250px; overflow-y: auto; font-family: monospace; font-size: 0.9em; margin-top: 20px; white-space: pre-wrap; }
        .log p { margin-bottom: 3px; line-height: 1.3; }
        .stats { display: grid; grid-template-columns: repeat(auto-fit, minmax(180px, 1fr)); gap: 20px; margin: 20px 0; }
        .stat-card { background: linear-gradient(135deg, #667eea, #764ba2); color: white; padding: 20px; border-radius: 15px; text-align: center; }
        .stat-value { font-size: 2rem; font-weight: bold; margin-bottom: 5px; }
        .canvas-container { position: relative; display: inline-block; margin: 10px; max-width: 100%; }
        .annotation-canvas { border: 2px solid #667eea; cursor: crosshair; display: block; max-width: 100%; height: auto; }
        /* NEW: Style for loss breakdown */
        .loss-breakdown span { margin-right: 15px; }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ü§ñ Local ML Training Platform</h1>
            <p>Train your models locally without any installations - Roboflow alternative that runs in your browser!</p>
        </div>

        <div class="tabs">
            <div class="tab active" onclick="switchTab(event, 'dataset')">üìÅ Dataset</div>
            <div class="tab" onclick="switchTab(event, 'labeling')">üè∑Ô∏è Labeling</div>
            <div class="tab" onclick="switchTab(event, 'training')">üß† Training</div>
            <div class="tab" onclick="switchTab(event, 'export')">üì§ Export</div>
        </div>

        <!-- Dataset Tab -->
        <div id="dataset" class="tab-content active">
            <h2>üìÅ Dataset Management</h2>
            <div class="file-upload-area">
                <div class="file-upload" id="combinedFileUpload">
                    <h3>Drop Images, Annotation Files (YOLO .txt, COCO .json), or Roboflow ZIP files here</h3>
                    <p>Supports: JPG, PNG, GIF, WEBP, TXT, JSON, ZIP</p>
                    <p>For ZIPs, ensure images and annotations (labels/ subfolder) are present.</p>
                    <input type="file" id="combinedFileInput" multiple accept="image/*,.txt,.json,.zip" style="display: none;">
                </div>
            </div>
             <div class="stats" id="datasetStats">
                <div class="stat-card"><div class="stat-value" id="totalImages">0</div><div>Total Images</div></div>
                <div class="stat-card"><div class="stat-value" id="labeledImages">0</div><div>Images with Annotations</div></div>
                <div class="stat-card"><div class="stat-value" id="totalAnnotations">0</div><div>Total Bounding Boxes</div></div>
                <div class="stat-card"><div class="stat-value" id="uniqueClasses">0</div><div>Unique Classes</div></div>
            </div>
            <div class="image-grid" id="imageGrid"></div>
        </div>

        <!-- Labeling Tab -->
        <div id="labeling" class="tab-content">
            <h2>üè∑Ô∏è Image Labeling</h2>
            <div class="model-info">
                <p><strong>Instructions:</strong> Click and drag to create bounding boxes. Enter labels. Or upload annotations in Dataset tab.</p>
                <button class="btn" onclick="nextImage()">Next Image (‚Üí)</button>
                <button class="btn" onclick="prevImage()">Previous Image (‚Üê)</button>
                <button class="btn" onclick="clearAnnotations()">Clear Current Image Annotations</button>
            </div>
            <div id="labelingArea" style="text-align: center;">
                <div class="canvas-container"><canvas id="labelingCanvas" class="annotation-canvas" width="800" height="600"></canvas></div>
            </div>
        </div>

        <!-- Training Tab -->
        <div id="training" class="tab-content">
             <h2>üß† Model Training</h2>
            <div class="model-info">
                <h3>Training Configuration</h3>
                <label for="modelType">Model Type:</label>
                <select id="modelType" onchange="updateTrainingUI()"> <!-- NEW: onchange handler -->
                    <option value="classification">Image Classification</option>
                    <option value="detection" selected>Object Detection</option>
                </select>
                <br><br>
                <label for="epochs">Epochs:</label>
                <input type="number" id="epochs" value="20" min="1" max="500"> <!-- Default epochs reduced for faster testing -->
                <br><br>
                <label for="learningRate">Learning Rate:</label>
                <input type="number" id="learningRate" value="0.001" step="0.0001" min="0.00001" max="0.1">
                <br><br>
                <!-- NEW: Batch size for detection -->
                <div id="batchSizeConfig" style="display: none;"> 
                    <label for="batchSize">Batch Size:</label>
                    <input type="number" id="batchSize" value="4" min="1" max="32">
                    <br><br>
                </div>
                <button class="btn" onclick="startTraining()" id="trainBtn">üöÄ Start Training</button>
                <button class="btn" onclick="stopTraining()" id="stopBtn" disabled>‚èπÔ∏è Stop Training</button>
            </div>

            <div class="progress-bar"><div class="progress-fill" id="trainingProgress"></div></div>

            <div class="stats" id="trainingStats">
                <div class="stat-card"><div class="stat-value" id="currentEpoch">0</div><div>Current Epoch</div></div>
                <div class="stat-card">
                    <div class="stat-value" id="trainingLoss">0.00</div>
                    <div>Total Loss</div>
                    <!-- NEW: Loss breakdown for detection -->
                    <div id="lossBreakdownDisplay" style="font-size: 0.8em; margin-top: 5px; display:none;">
                        <span id="locLossDisplay">Loc: 0.00</span>
                        <span id="classLossDisplay">Class: 0.00</span>
                    </div>
                </div>
                <div class="stat-card">
                    <div class="stat-value" id="accuracy">0.00%</div>
                    <div id="accuracyLabel">Accuracy (Metric)</div>
                </div>
            </div>
            <div class="log" id="trainingLog"></div>
        </div>

        <!-- Export Tab -->
        <div id="export" class="tab-content">
             <h2>üì§ Export Model</h2>
            <div class="model-info">
                <h3>Export Options</h3>
                <button class="btn" onclick="exportModel('tensorflow')">üì¶ Export TensorFlow.js</button>
                <button class="btn" onclick="exportModel('weights')">‚öñÔ∏è Export Weights Only</button>
                <button class="btn" onclick="exportModel('onnx')">üîÑ Export ONNX (Simulated)</button>
                <button class="btn" onclick="exportDataset()">üìä Export Labeled Dataset (JSON)</button>
            </div>
            <div id="exportStatus"></div>
        </div>
    </div>

    <script>
        // Global variables and JSZip check from previous version...
        let images = []; 
        let currentImageIndex = 0;
        let model = null;
        let isTraining = false;
        let classMap = {}; // className: integerId
        let nextClassId = 0;
        const IMG_WIDTH = 224; // Fixed input size for models
        const IMG_HEIGHT = 224;

        if (typeof JSZip === 'undefined') {
            alert("JSZip library not loaded. ZIP file processing will not work. Please check your internet connection or the JSZip CDN link.");
        }
        // --- Dataset Management, Labeling System (mostly same as your v3) ---
        // These will be kept concise here for brevity, assuming they are largely functional from the previous step.
        // Key functions: handleCombinedDrop, processZipFile, processImageFiles, processAnnotationFiles,
        // parseYoloTxtAnnotations, parseCocoJsonAnnotations, updateDatasetStats, renderImageGrid,
        // setCurrentImage, loadImageForLabeling, redrawCanvas, handleCanvasMouseDown/Move/Up, drawBoundingBox,
        // nextImage, prevImage, clearAnnotations.
        // For brevity, I'll skip pasting all of that, but it's assumed to be present and working.
        // I will include the stubs for functions that need to be present.

        const combinedFileUpload = document.getElementById('combinedFileUpload');
        const combinedFileInput = document.getElementById('combinedFileInput');
        combinedFileUpload.addEventListener('click', () => combinedFileInput.click());
        combinedFileUpload.addEventListener('dragover', (e) => handleDragOver(e, combinedFileUpload));
        combinedFileUpload.addEventListener('dragleave', (e) => handleDragLeave(e, combinedFileUpload));
        combinedFileUpload.addEventListener('drop', handleCombinedDrop);
        combinedFileInput.addEventListener('change', handleCombinedFileSelect);
        function handleDragOver(e, element) { e.preventDefault(); element.classList.add('dragover'); }
        function handleDragLeave(e, element) { element.classList.remove('dragover'); }
        async function handleCombinedDrop(e) { e.preventDefault(); combinedFileUpload.classList.remove('dragover'); await processCombinedFiles(Array.from(e.dataTransfer.files));}
        async function handleCombinedFileSelect(e) { await processCombinedFiles(Array.from(e.target.files));}
        
        async function processCombinedFiles(files) { /* ... As in previous version ... */ 
            log("Processing uploaded files...");
            const imageFiles = [], yoloTxtFiles = [], cocoJsonFiles = [], zipFiles = [];
            for (const file of files) {
                if (file.type.startsWith('image/')) imageFiles.push(file);
                else if (file.name.endsWith('.txt')) yoloTxtFiles.push(file);
                else if (file.name.endsWith('.json')) cocoJsonFiles.push(file);
                else if (file.name.endsWith('.zip')) zipFiles.push(file);
                else log(`‚ö†Ô∏è Unsupported file type: ${file.name}`);
            }
            for (const zipFile of zipFiles) await processZipFile(zipFile);
            if (imageFiles.length > 0) await processImageFiles(imageFiles);
            if (yoloTxtFiles.length > 0) await processAnnotationFiles(yoloTxtFiles, 'yolo');
            if (cocoJsonFiles.length > 0) await processAnnotationFiles(cocoJsonFiles, 'coco');
            updateDatasetStats(); renderImageGrid();
            if (images.length > 0 && currentImageIndex >= images.length) currentImageIndex = 0;
            if (images.length > 0) loadImageForLabeling();
        }
        async function processZipFile(zipFile) { /* ... As in previous version ... */ 
            log(`‚öôÔ∏è Processing ZIP: ${zipFile.name}`);
            const jszip = new JSZip(); const zip = await jszip.loadAsync(zipFile);
            const imageFilesFromZip = [], yoloTxtFilesFromZip = []; let cocoJsonFileFromZip = null;
            for (const filePath in zip.files) {
                const fileEntry = zip.files[filePath];
                if (!fileEntry.dir) {
                    const fileName = filePath.split('/').pop(); const ext = fileName.split('.').pop().toLowerCase();
                    if (['jpg', 'jpeg', 'png', 'webp'].includes(ext)) imageFilesFromZip.push(new File([await fileEntry.async('blob')], fileName));
                    else if (ext === 'txt') yoloTxtFilesFromZip.push({ name: fileName, content: await fileEntry.async('string')});
                    else if (fileName.endsWith('.coco.json') || fileName === 'annotations.json') cocoJsonFileFromZip = { name: fileName, content: await fileEntry.async('string')};
                }
            }
            if (imageFilesFromZip.length > 0) await processImageFiles(imageFilesFromZip);
            if (cocoJsonFileFromZip) await parseCocoJsonAnnotations(cocoJsonFileFromZip.content);
            else if (yoloTxtFilesFromZip.length > 0) {
                for (const yoloFile of yoloTxtFilesFromZip) {
                    const imgNamePart = yoloFile.name.replace(/\.[^/.]+$/, "");
                    const targetImg = images.find(img => img.name.startsWith(imgNamePart + '.'));
                    if (targetImg) parseYoloTxtAnnotations(yoloFile.content, targetImg);
                }
            }
            log(`‚úÖ ZIP ${zipFile.name} processed.`);
        }
        async function processImageFiles(files) { /* ... As in previous version ... */ 
             for (const file of files) {
                if (images.find(img => img.id === file.name)) continue;
                const reader = new FileReader();
                await new Promise(resolve => {
                    reader.onload = e_reader => {
                        const tempImg = new Image();
                        tempImg.onload = () => images.push({ id: file.name, name: file.name, src: e_reader.target.result, annotations: [], originalWidth: tempImg.width, originalHeight: tempImg.height, tensor: null });
                        tempImg.src = e_reader.target.result; resolve();
                    }; reader.readAsDataURL(file);
                });
            }
        }
        async function processAnnotationFiles(files, format) { /* ... As in previous version ... */ 
            for (const file of files) {
                const reader = new FileReader();
                await new Promise(resolve => {
                    reader.onload = e_reader => {
                        const content = e_reader.target.result;
                        if (format === 'yolo') {
                            const imgNamePart = file.name.replace(/\.[^/.]+$/, "");
                            const targetImg = images.find(img => img.name.startsWith(imgNamePart + '.'));
                            if (targetImg) parseYoloTxtAnnotations(content, targetImg);
                        } else if (format === 'coco') parseCocoJsonAnnotations(content);
                        resolve();
                    }; reader.readAsText(file);
                });
            }
        }
        function parseYoloTxtAnnotations(txtContent, imageObject) { /* ... As in previous version ... */
            if (!imageObject) return; imageObject.annotations = []; 
            txtContent.trim().split('\n').forEach(line => {
                const p = line.trim().split(' '); if (p.length !== 5) return;
                const classIdFromFile = parseInt(p[0]); let label = `class_${classIdFromFile}`;
                for (const cn in classMap) if (classMap[cn] === classIdFromFile) label = cn;
                if (!Object.values(classMap).includes(classIdFromFile) && !classMap[label]) classMap[label] = classIdFromFile;
                if (classIdFromFile >= nextClassId) nextClassId = classIdFromFile +1;
                const w = parseFloat(p[3]) * imageObject.originalWidth, h = parseFloat(p[4]) * imageObject.originalHeight;
                const x = parseFloat(p[1]) * imageObject.originalWidth - w/2, y = parseFloat(p[2]) * imageObject.originalHeight - h/2;
                imageObject.annotations.push({ x, y, width: w, height: h, label, classId: classIdFromFile });
            });
        }
        function parseCocoJsonAnnotations(jsonContent) { /* ... As in previous version ... */ 
            try {
                const coco = JSON.parse(jsonContent); classMap = {}; nextClassId = 0;
                coco.categories?.forEach(cat => { classMap[cat.name] = cat.id; if(cat.id >= nextClassId) nextClassId = cat.id + 1; });
                coco.images?.forEach(ci => {
                    let imgObj = images.find(img => img.name === ci.file_name || img.id === ci.file_name);
                    if(imgObj) {
                        imgObj.originalWidth = ci.width || imgObj.originalWidth; imgObj.originalHeight = ci.height || imgObj.originalHeight;
                        imgObj.annotations = [];
                        coco.annotations?.filter(ann => ann.image_id === ci.id).forEach(ca => {
                            const [x,y,w,h] = ca.bbox; let catName = "unknown";
                            const cat = coco.categories.find(c => c.id === ca.category_id); if(cat) catName = cat.name;
                            imgObj.annotations.push({x,y,width:w,height:h, label:catName, classId: ca.category_id});
                        });
                    }
                });
            } catch (e) { log(`‚ùå Error parsing COCO: ${e.message}`); console.error(e); }
        }
        function updateDatasetStats() { /* ... As in previous version ... */ 
            const labeledImgCount = images.filter(i=>i.annotations?.length>0).length;
            const totalBBoxes = images.reduce((s,i)=>s+(i.annotations?.length||0),0);
            const currentClasses = new Set(Object.keys(classMap));
            document.getElementById('totalImages').textContent=images.length;
            document.getElementById('labeledImages').textContent=labeledImgCount;
            document.getElementById('totalAnnotations').textContent=totalBBoxes;
            document.getElementById('uniqueClasses').textContent=currentClasses.size;
        }
        function renderImageGrid() { /* ... As in previous version ... */ 
            const grid = document.getElementById('imageGrid'); grid.innerHTML = '';
            images.forEach((img,idx) => {
                const item = document.createElement('div'); item.className = 'image-item';
                let badge = img.annotations?.length > 0 ? `<span class="annotation-count-badge">${img.annotations.length}</span>` : '';
                item.innerHTML = `<img src="${img.src}" alt="${img.name}" title="${img.name}">${badge}<div class="image-controls"><button class="btn" onclick="setCurrentImage(${idx})">Label/View</button><button class="btn" onclick="removeImage(${idx})">Remove</button></div>`;
                grid.appendChild(item);
            });
        }
        function removeImage(index) { /* ... As in previous version ... */ }
        const canvas = document.getElementById('labelingCanvas'), ctx = canvas.getContext('2d');
        let isDrawing = false, startX, startY, currentImgElement = null;
        canvas.addEventListener('mousedown', handleCanvasMouseDown); canvas.addEventListener('mousemove', handleCanvasMouseMove); canvas.addEventListener('mouseup', handleCanvasMouseUp);
        function setCurrentImage(index) { /* ... As in previous version ... */ if(index >=0 && index < images.length){currentImageIndex=index; switchTab(null,'labeling'); loadImageForLabeling();} else if (images.length===0){currentImageIndex=0;switchTab(null,'labeling');loadImageForLabeling();}}
        function loadImageForLabeling() { /* ... As in previous version, ensure responsive canvas size ... */ 
            if (images.length === 0 || !images[currentImageIndex]) { ctx.clearRect(0,0,canvas.width,canvas.height); ctx.fillText("No image.",10,20); currentImgElement=null; return; }
            const imgData = images[currentImageIndex]; currentImgElement = new Image();
            currentImgElement.onload = () => {
                const cc = canvas.parentElement; const maxW = cc.clientWidth - 10, maxH = 500;
                let w = currentImgElement.naturalWidth, h = currentImgElement.naturalHeight, r = w/h;
                if (w > maxW) { w = maxW; h = w/r; } if (h > maxH) { h = maxH; w = h*r; }
                canvas.width = w; canvas.height = h; redrawCanvas();
            }; currentImgElement.src = imgData.src;
        }
        function redrawCanvas() { /* ... As in previous version ... */ 
            if(!canvas || !ctx) return; ctx.clearRect(0,0,canvas.width,canvas.height); if(!currentImgElement || !currentImgElement.complete || currentImgElement.naturalWidth === 0) return;
            ctx.drawImage(currentImgElement,0,0,canvas.width,canvas.height); const imgData = images[currentImageIndex];
            if(imgData?.annotations && imgData.originalWidth){ const scX = canvas.width/imgData.originalWidth, scY = canvas.height/imgData.originalHeight; imgData.annotations.forEach(a => drawBoundingBox(a.x*scX, a.y*scY, a.width*scX, a.height*scY, a.label));}
        }
        function handleCanvasMouseDown(e) { /* ... As in previous version ... */ if(!currentImgElement) return; isDrawing=true; const r=canvas.getBoundingClientRect();startX=e.clientX-r.left;startY=e.clientY-r.top;}
        function handleCanvasMouseMove(e) { /* ... As in previous version ... */ if(!isDrawing||!currentImgElement)return; const r=canvas.getBoundingClientRect(),cX=e.clientX-r.left,cY=e.clientY-r.top; redrawCanvas(); drawBoundingBox(startX,startY,cX-startX,cY-startY,'Drawing...');}
        function handleCanvasMouseUp(e) { /* ... As in previous version ... */
            if(!isDrawing||!currentImgElement)return; isDrawing=false; const r=canvas.getBoundingClientRect(),eX=e.clientX-r.left,eY=e.clientY-r.top;
            const cW=eX-startX,cH=eY-startY; if(Math.abs(cW)<5||Math.abs(cH)<5){redrawCanvas();return;}
            const l=prompt('Label:'); if(l&&l.trim()!==""){const iD=images[currentImageIndex]; if(!iD.originalWidth){redrawCanvas();return;} const sX=iD.originalWidth/canvas.width,sY=iD.originalHeight/canvas.height; const oX=Math.min(startX,eX)*sX,oY=Math.min(startY,eY)*sY,oW=Math.abs(cW)*sX,oH=Math.abs(cH)*sY; const newAnn={x:oX,y:oY,width:oW,height:oH,label:l.trim()}; iD.annotations.push(newAnn); if(!(l.trim() in classMap)) classMap[l.trim()]=nextClassId++; newAnn.classId=classMap[l.trim()]; updateDatasetStats(); redrawCanvas();} else redrawCanvas();
        }
        function drawBoundingBox(x,y,w,h,l){/* ... As in previous version ... */ ctx.strokeStyle='#667eea';ctx.lineWidth=2;ctx.strokeRect(x,y,w,h);ctx.fillStyle='#667eea';ctx.globalAlpha=0.7;const tM=ctx.measureText(l),tW=Math.max(30,tM.width+10),tH=20;ctx.fillRect(x,y-tH,tW,tH);ctx.globalAlpha=1.0;ctx.fillStyle='#fff';ctx.font='14px Segoe UI';ctx.textBaseline='middle';ctx.fillText(l,x+5,y-(tH/2));}
        function nextImage(){if(images.length===0)return;currentImageIndex=(currentImageIndex+1)%images.length;loadImageForLabeling();}
        function prevImage(){if(images.length===0)return;currentImageIndex=(currentImageIndex-1+images.length)%images.length;loadImageForLabeling();}
        function clearAnnotations(){if(images.length>0&&images[currentImageIndex]?.annotations){images[currentImageIndex].annotations=[];redrawCanvas();updateDatasetStats();log(`Annotations cleared for ${images[currentImageIndex].name}`);}}

        // --- NEW: Training System ---
        function updateTrainingUI() {
            const modelType = document.getElementById('modelType').value;
            const batchSizeConfig = document.getElementById('batchSizeConfig');
            const accuracyLabel = document.getElementById('accuracyLabel');
            const lossBreakdownDisplay = document.getElementById('lossBreakdownDisplay');

            if (modelType === 'detection') {
                batchSizeConfig.style.display = 'block';
                accuracyLabel.textContent = 'mAP (Simulated)'; // Or a real metric if implemented
                lossBreakdownDisplay.style.display = 'block';
            } else { // Classification
                batchSizeConfig.style.display = 'none';
                accuracyLabel.textContent = 'Accuracy';
                lossBreakdownDisplay.style.display = 'none';
            }
        }
        
        // Call it once on load to set initial UI state
        document.addEventListener('DOMContentLoaded', updateTrainingUI);


        async function startTraining() {
            if (images.length === 0) { alert('Please upload dataset!'); return; }
            const imagesWithAnn = images.filter(img => img.annotations && img.annotations.length > 0);
            if (imagesWithAnn.length === 0) { alert('Please annotate images or upload annotations!'); return; }

            isTraining = true;
            document.getElementById('trainBtn').disabled = true;
            document.getElementById('stopBtn').disabled = false;
            log('üöÄ Training session started...');

            const modelType = document.getElementById('modelType').value;
            const epochs = parseInt(document.getElementById('epochs').value);
            const learningRate = parseFloat(document.getElementById('learningRate').value);
            const batchSize = modelType === 'detection' ? parseInt(document.getElementById('batchSize').value) : null;


            // Rebuild classMap based on all current annotations to ensure it's up-to-date
            classMap = {}; nextClassId = 0;
            const uniqueLabelSet = new Set();
            imagesWithAnn.forEach(img => img.annotations.forEach(ann => uniqueLabelSet.add(ann.label)));
            Array.from(uniqueLabelSet).sort().forEach(label => { // Sort for consistent ID assignment
                if (!(label in classMap)) classMap[label] = nextClassId++;
            });
            const numClasses = Object.keys(classMap).length;

            log(`Model Type: ${modelType}, Epochs: ${epochs}, LR: ${learningRate}${batchSize ? ', Batch: '+batchSize : ''}`);
            log(`Training with ${imagesWithAnn.length} annotated images and ${numClasses} unique classes.`);
            log(`Class Map: ${JSON.stringify(classMap)}`);

            if (numClasses === 0) {
                alert("No classes found in annotations. Cannot start training.");
                stopTraining(); return;
            }

            try {
                tf.dispose(model); // Dispose any previous model
                model = null;

                if (modelType === 'classification') {
                    document.getElementById('accuracyLabel').textContent = 'Accuracy';
                    document.getElementById('lossBreakdownDisplay').style.display = 'none';
                    await trainClassificationModel(epochs, learningRate, imagesWithAnn, numClasses);
                } else { // Object Detection
                    document.getElementById('accuracyLabel').textContent = 'Avg Loss'; // More realistic than sim mAP for now
                     document.getElementById('lossBreakdownDisplay').style.display = 'block';
                    await trainSimpleDetectionModel(epochs, learningRate, batchSize, imagesWithAnn, numClasses);
                }
            } catch (error) {
                log(`‚ùå Training Error: ${error.message}`); console.error("Training error details:", error);
            } finally {
                isTraining = false; // Ensure training flag is reset
                document.getElementById('trainBtn').disabled = false;
                document.getElementById('stopBtn').disabled = true;
                log(isTraining ? '‚ö†Ô∏è Training loop exited unexpectedly.' : 'üèÅ Training session ended.');
            }
        }

        // Classification Model (largely same as your v3, with minor cleanups)
        async function trainClassificationModel(epochs, learningRate, trainingImages, numOutputClasses) { /* ... As in previous version ... */
            log('üîß Building classification model...');
            model = tf.sequential(); /* ... model definition ... */
            model.add(tf.layers.conv2d({inputShape: [IMG_HEIGHT, IMG_WIDTH, 3], filters: 16, kernelSize: 3, activation: 'relu', padding: 'same'}));
            model.add(tf.layers.maxPooling2d({poolSize: 2}));
            model.add(tf.layers.conv2d({filters: 32, kernelSize: 3, activation: 'relu', padding: 'same'}));
            model.add(tf.layers.maxPooling2d({poolSize: 2}));
            model.add(tf.layers.conv2d({filters: 64, kernelSize: 3, activation: 'relu', padding: 'same'}));
            model.add(tf.layers.maxPooling2d({poolSize: 2}));
            model.add(tf.layers.flatten());
            model.add(tf.layers.dense({units: 128, activation: 'relu'}));
            model.add(tf.layers.dropout({rate: 0.3}));
            model.add(tf.layers.dense({units: numOutputClasses, activation: 'softmax'}));

            model.compile({optimizer: tf.train.adam(learningRate), loss: 'categoricalCrossentropy', metrics: ['accuracy']});
            log('üìä Preparing classification data...');
            const {xs, ys} = await prepareClassificationData(trainingImages, numOutputClasses);
            if (!xs || !ys) { log("Failed to prepare classification data."); return; }
            log(`Data shapes: xs=${xs.shape}, ys=${ys.shape}`);
            log('üéØ Starting classification training loop...');
            try {
                await model.fit(xs, ys, {
                    epochs: epochs, batchSize: 8, shuffle: true, // Added batchSize
                    callbacks: {
                        onEpochBegin: async (ep) => { document.getElementById('currentEpoch').textContent = ep + 1; await tf.nextFrame(); },
                        onEpochEnd: (ep, logs) => {
                            if (!isTraining) model.stopTraining = true;
                            const prog = ((ep + 1) / epochs) * 100;
                            document.getElementById('trainingProgress').style.width = prog + '%';
                            document.getElementById('trainingLoss').textContent = logs.loss.toFixed(4);
                            document.getElementById('accuracy').textContent = (logs.acc * 100).toFixed(2) + '%';
                            log(`Ep ${ep+1}/${epochs} - Loss: ${logs.loss.toFixed(4)}, Acc: ${(logs.acc*100).toFixed(2)}%`);
                        }
                    }
                });
            } finally { tf.dispose([xs, ys]); } // Dispose tensors
            if(isTraining) log('‚úÖ Classification training completed!');
        }

        async function prepareClassificationData(trainingImages, numOutputClasses) {
            const tensors = await Promise.all(trainingImages.map(async imgData => {
                if (!imgData.annotations || imgData.annotations.length === 0) return null;
                const imageElement = new Image(); imageElement.src = imgData.src;
                await new Promise(r => imageElement.onload = r);
                return tf.tidy(() => {
                    const imageTensor = tf.browser.fromPixels(imageElement).resizeNearestNeighbor([IMG_HEIGHT, IMG_WIDTH]).toFloat().div(255.0);
                    const label = imgData.annotations[0].label; // Use first annotation's label
                    const classId = classMap[label];
                    const labelTensor = tf.oneHot(tf.tensor1d([classId], 'int32'), numOutputClasses).squeeze();
                    return { imageTensor, labelTensor };
                });
            }));

            const validTensors = tensors.filter(t => t !== null);
            if (validTensors.length === 0) return {};
            const xs = tf.stack(validTensors.map(t => t.imageTensor));
            const ys = tf.stack(validTensors.map(t => t.labelTensor));
            validTensors.forEach(t => { t.imageTensor.dispose(); t.labelTensor.dispose(); }); // Dispose intermediate
            return { xs, ys };
        }


        // --- NEW: Simplified Object Detection Model and Training ---
        function createObjectDetectionModel(numClasses) {
            const input = tf.input({shape: [IMG_HEIGHT, IMG_WIDTH, 3]});
            
            // Simple CNN Backbone
            let x = tf.layers.conv2d({filters: 16, kernelSize: 3, activation: 'relu', padding: 'same'}).apply(input);
            x = tf.layers.maxPooling2d({poolSize: 2}).apply(x); // 112x112
            x = tf.layers.conv2d({filters: 32, kernelSize: 3, activation: 'relu', padding: 'same'}).apply(x);
            x = tf.layers.maxPooling2d({poolSize: 2}).apply(x); // 56x56
            x = tf.layers.conv2d({filters: 64, kernelSize: 3, activation: 'relu', padding: 'same'}).apply(x);
            x = tf.layers.maxPooling2d({poolSize: 2}).apply(x); // 28x28
            x = tf.layers.flatten().apply(x);
            x = tf.layers.dense({units: 128, activation: 'relu'}).apply(x);
            x = tf.layers.dropout({rate: 0.2}).apply(x);

            // Output Heads
            const bboxOutput = tf.layers.dense({units: 4, activation: 'sigmoid', name: 'bbox_output'}).apply(x); // 4 for normalized (x,y,w,h) between 0-1
            const classOutput = tf.layers.dense({units: numClasses, activation: 'softmax', name: 'class_output'}).apply(x);
            
            const detModel = tf.model({inputs: input, outputs: [bboxOutput, classOutput]});
            return detModel;
        }

        function detectionLossFunction(yTrue, yPred) {
            return tf.tidy(() => {
                const trueBoxes = yTrue.slice([0, 0], [-1, 4]); // First 4 are boxes
                const trueClasses = yTrue.slice([0, 4], [-1, -1]); // Rest are classes
                
                const predBoxes = yPred[0]; // Model output for boxes
                const predClasses = yPred[1]; // Model output for classes

                const locLoss = tf.losses.meanSquaredError(trueBoxes, predBoxes);
                const classLoss = tf.losses.categoricalCrossentropy(trueClasses, predClasses);
                
                // You might want to weigh these losses, e.g., tf.add(locLoss.mul(alpha), classLoss)
                const totalLoss = tf.add(locLoss, classLoss); 
                return totalLoss; // Return mean loss over the batch
            });
        }
        
        // Store individual losses for display
        let currentLocLoss = 0;
        let currentClassLoss = 0;

        async function trainSimpleDetectionModel(epochs, learningRate, batchSize, trainingImages, numClasses) {
            log('üîß Building Simple Object Detection model...');
            model = createObjectDetectionModel(numClasses);
            
            // Custom optimizer instance to potentially use things like clipnorm if needed
            const optimizer = tf.train.adam(learningRate); 

            // We need a custom training loop for multiple outputs and custom loss handling in TFJS effectively for logging
            log('üìä Preparing all detection data (can take a moment for many images)...');
            const {allXs, allYs} = await prepareDetectionData(trainingImages, numClasses);

            if (!allXs || allXs.shape[0] === 0) {
                log("‚ùå No valid data prepared for detection training.");
                tf.dispose([allXs, allYs]);
                return;
            }
            log(`Prepared detection data: Xs shape: ${allXs.shape}, Ys shape: ${allYs.shape}`);


            log('üéØ Starting Detection training loop...');
            for (let epoch = 0; epoch < epochs; epoch++) {
                if (!isTraining) { log("Training stopped by user."); break; }
                document.getElementById('currentEpoch').textContent = epoch + 1;
                let epochTotalLoss = 0;
                let epochLocLoss = 0;
                let epochClassLoss = 0;
                let numBatches = 0;

                // Shuffle data indices for each epoch
                const numSamples = allXs.shape[0];
                const shuffledIndices = tf.util.createShuffledIndices(numSamples);
                
                for (let i = 0; i < numSamples; i += batchSize) {
                    if (!isTraining) break;
                    const batchIndices = Array.from(shuffledIndices.slice(i, Math.min(i + batchSize, numSamples)));
                    
                    const xsBatch = tf.gather(allXs, tf.tensor1d(batchIndices, 'int32'));
                    const ysBatchTrueBoxes = tf.gather(allYs.trueBoxes, tf.tensor1d(batchIndices, 'int32'));
                    const ysBatchTrueClasses = tf.gather(allYs.trueClasses, tf.tensor1d(batchIndices, 'int32'));

                    const { grads, lossValues } = optimizer.minimize(() => {
                        const predYs = model.predict(xsBatch); // predYs will be [predBoxesTensor, predClassesTensor]
                        const predBoxes = predYs[0];
                        const predClasses = predYs[1];

                        // Calculate individual losses for logging and then the combined loss for optimization
                        const locLoss = tf.losses.meanSquaredError(ysBatchTrueBoxes, predBoxes).mean();
                        const classLoss = tf.losses.categoricalCrossentropy(ysBatchTrueClasses, predClasses).mean();
                        
                        currentLocLoss = locLoss.dataSync()[0]; // For UI update
                        currentClassLoss = classLoss.dataSync()[0]; // For UI update
                        
                        // Combined loss for gradient calculation
                        const totalLoss = tf.add(locLoss.mul(0.5), classLoss.mul(0.5)); // Example: equal weight
                        return totalLoss;
                    }, true, model.trainableWeights); // true for returnCost, then varList

                    // Dispose gradients and batch tensors
                    Object.values(grads).forEach(grad => grad.dispose());
                    tf.dispose([xsBatch, ysBatchTrueBoxes, ysBatchTrueClasses]);
                    // Note: predYs tensors from model.predict are disposed automatically by TFJS if not returned from tidy/kept

                    const batchTotalLoss = (currentLocLoss + currentClassLoss) / 2; // Reflects the weighted sum if weights change
                    epochTotalLoss += batchTotalLoss;
                    epochLocLoss += currentLocLoss;
                    epochClassLoss += currentClassLoss;
                    numBatches++;
                    
                    await tf.nextFrame(); // Yield to UI
                }
                shuffledIndices.dispose(); // Dispose shuffled indices tensor

                if (numBatches > 0) {
                    const avgEpochLoss = epochTotalLoss / numBatches;
                    const avgEpochLocLoss = epochLocLoss / numBatches;
                    const avgEpochClassLoss = epochClassLoss / numBatches;

                    document.getElementById('trainingLoss').textContent = avgEpochLoss.toFixed(4);
                    document.getElementById('locLossDisplay').textContent = `Loc: ${avgEpochLocLoss.toFixed(4)}`;
                    document.getElementById('classLossDisplay').textContent = `Class: ${avgEpochClassLoss.toFixed(4)}`;
                    // For "Accuracy" in detection, we'd typically use mAP, which is complex to calculate here.
                    // Using average loss as a proxy for now.
                    document.getElementById('accuracy').textContent = avgEpochLoss.toFixed(4); 
                    log(`Ep ${epoch+1}/${epochs} - AvgLoss: ${avgEpochLoss.toFixed(4)} (Loc: ${avgEpochLocLoss.toFixed(4)}, Class: ${avgEpochClassLoss.toFixed(4)})`);
                }
                const progress = ((epoch + 1) / epochs) * 100;
                document.getElementById('trainingProgress').style.width = progress + '%';
                if (!isTraining) break;
            }
            
            tf.dispose([allXs, allYs.trueBoxes, allYs.trueClasses]); // Dispose dataset tensors
            if(isTraining) log('‚úÖ Simple Detection training completed!');
        }

        async function prepareDetectionData(trainingImages, numClasses) {
            const imageTensors = [];
            const targetBoxes = [];
            const targetClasses = [];
            let count = 0;

            for (const imgData of trainingImages) {
                if (!imgData.annotations || imgData.annotations.length === 0) continue;
                
                // Load image only once
                const imageElement = new Image();
                imageElement.src = imgData.src;
                await new Promise(r => imageElement.onload = r);
                
                const originalImageTensor = tf.tidy(()=>tf.browser.fromPixels(imageElement).toFloat().div(255.0));
                const resizedImageTensor = tf.image.resizeNearestNeighbor(originalImageTensor, [IMG_HEIGHT, IMG_WIDTH]);
                originalImageTensor.dispose(); // Dispose original full-size tensor

                for (const ann of imgData.annotations) {
                    imageTensors.push(resizedImageTensor.clone()); // Clone for each annotation

                    // Normalize box: x, y, width, height (relative to original image dimensions)
                    const normX = ann.x / imgData.originalWidth;
                    const normY = ann.y / imgData.originalHeight;
                    const normW = ann.width / imgData.originalWidth;
                    const normH = ann.height / imgData.originalHeight;
                    targetBoxes.push(tf.tensor1d([normX, normY, normW, normH]));

                    const classId = classMap[ann.label];
                    targetClasses.push(tf.oneHot(tf.tensor1d([classId], 'int32'), numClasses).squeeze());
                    count++;
                }
                resizedImageTensor.dispose(); // Dispose after all its annotations processed
            }
            
            if (imageTensors.length === 0) return {};

            log(`Processed ${count} annotations for detection training.`);
            const allXs = tf.stack(imageTensors);
            const allYs = {
                trueBoxes: tf.stack(targetBoxes),
                trueClasses: tf.stack(targetClasses)
            };
            
            imageTensors.forEach(t => t.dispose());
            targetBoxes.forEach(t => t.dispose());
            targetClasses.forEach(t => t.dispose());
            
            return {allXs, allYs};
        }


        function stopTraining() { /* ... As in previous version ... */ 
            isTraining = false;
            if (model && typeof model.stopTraining === 'boolean') { model.stopTraining = true; }
            log('‚èπÔ∏è Training stop requested.');
            document.getElementById('trainBtn').disabled = false;
            document.getElementById('stopBtn').disabled = true;
        }

        function log(message) { /* ... As in previous version ... */ 
            const logDiv=document.getElementById('trainingLog');
            const ts=new Date().toLocaleTimeString([],{hour:'2-digit',minute:'2-digit',second:'2-digit'});
            const p=document.createElement('p');p.textContent=`[${ts}] ${message}`;
            logDiv.appendChild(p);logDiv.scrollTop=logDiv.scrollHeight;
        }

        // Export Functions
        async function exportModel(format) { /* ... As in previous version, ensure async data for weights ... */
            if(!model){alert('No model trained!');return;} const status=document.getElementById('exportStatus');status.innerHTML='Processing...';
            try{
                if(format==='tensorflow'){await model.save('downloads://local-ml-detection-model');status.innerHTML='‚úÖ TF.js Model Exported!';}
                else if(format==='weights'){
                    const w=model.getWeights(); const wd = []; for(const t of w){wd.push(Array.from(await t.data())); t.dispose();}
                    downloadBlob(new Blob([JSON.stringify(wd,null,2)],{type:'application/json'}),'detection-model-weights.json'); status.innerHTML='‚úÖ Weights Exported!';
                } else { status.innerHTML='üöß ONNX Placeholder.';}
            }catch(e){status.innerHTML=`‚ùå Export Failed: ${e.message}`;log(e.message);console.error(e);}
        }
        function exportDataset() { /* ... As in previous version ... */ }
        function downloadBlob(blob, filename) {const u=URL.createObjectURL(blob),a=document.createElement('a');a.style.display='none';a.href=u;a.download=filename;document.body.appendChild(a);a.click();URL.revokeObjectURL(u);a.remove();}
        
        // Tab Management and Initialization
        function switchTab(event, tabName) { /* ... As in previous version ... */ 
             document.querySelectorAll('.tab-content').forEach(c=>c.classList.remove('active'));
             document.querySelectorAll('.tab').forEach(t=>t.classList.remove('active'));
             document.getElementById(tabName).classList.add('active');
             const targetTabButton = event ? event.target : document.querySelector(`.tab[onclick*="'${tabName}'"]`);
             if(targetTabButton) targetTabButton.classList.add('active');
             if(tabName==='labeling') loadImageForLabeling();
             updateTrainingUI(); // Update UI elements like batch size visibility
        }
        document.addEventListener('DOMContentLoaded', () => { /* ... Load from localStorage, update stats ... */ 
            log('üéâ Platform Ready!'); updateTrainingUI();
            try { /* Load localStorage */ } catch(e) { /* handle */ }
            updateDatasetStats(); renderImageGrid(); if(images.length>0) loadImageForLabeling();
        });
        setInterval(() => { /* ... Autosave ... */ }, 15000);
        ['dragenter','dragover','dragleave','drop'].forEach(ev=>document.addEventListener(ev,e=>{e.preventDefault();e.stopPropagation();}));
        document.addEventListener('keydown', e => { /* ... Keyboard shortcuts ... */ 
            const activeTab = document.querySelector('.tab-content.active');
            if (activeTab && activeTab.id === 'labeling') {
                if (document.activeElement.tagName === 'INPUT' || document.activeElement.tagName === 'TEXTAREA') return;
                if (e.key === 'ArrowRight') { e.preventDefault(); nextImage(); }
                else if (e.key === 'ArrowLeft') { e.preventDefault(); prevImage(); }
            }
        });

    </script>
</body>
</html>
